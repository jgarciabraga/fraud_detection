{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model.sav'\n",
    "df = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "5  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427 -0.232794   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "5  0.105915  0.253844  0.081080    3.67      0  \n",
       "\n",
       "[6 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first 6 rows from data frame\n",
    "#the inout values are the V columns\n",
    "#the variable of interest is Class column - (0 non fraud - 1 fraud)\n",
    "#data set from kagle (https://www.kaggle.com/mlg-ulb/creditcardfraud/data#) \n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      float64\n",
       "V1        float64\n",
       "V2        float64\n",
       "V3        float64\n",
       "V4        float64\n",
       "V5        float64\n",
       "V6        float64\n",
       "V7        float64\n",
       "V8        float64\n",
       "V9        float64\n",
       "V10       float64\n",
       "V11       float64\n",
       "V12       float64\n",
       "V13       float64\n",
       "V14       float64\n",
       "V15       float64\n",
       "V16       float64\n",
       "V17       float64\n",
       "V18       float64\n",
       "V19       float64\n",
       "V20       float64\n",
       "V21       float64\n",
       "V22       float64\n",
       "V23       float64\n",
       "V24       float64\n",
       "V25       float64\n",
       "V26       float64\n",
       "V27       float64\n",
       "V28       float64\n",
       "Amount    float64\n",
       "Class       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = df.iloc[:,1:30].columns\n",
    "outout_columns = df.iloc[:, 30:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
      "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
      "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(input_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Class'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(outout_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the input (x data) and the output (y data) data\n",
    "input_data = df[input_columns]\n",
    "output_data = df[outout_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, test_input, train_output, test_output = train_test_split(input_data, output_data, train_size=0.8, test_size=0.2, random_state=1)\n",
    "train_input = (train_input-train_input.mean())/train_input.std()\n",
    "test_input = (test_input-test_input.mean())/test_input.std()\n",
    "train_input = train_input.values\n",
    "train_output = train_output.values.flatten()\n",
    "test_input = test_input.values\n",
    "test_output = test_output.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR = LogisticRegression(random_state=1)\n",
    "model_LR.fit(train_input, train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_over_test = model_LR.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score = 0.9991397773954567\n",
      "f1_socre = 0.6620689655172414\n",
      "precision_score = 0.8275862068965517\n",
      "recall_score = 0.5517241379310345\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy_score = {accuracy_score(test_output, prediction_over_test)}')\n",
    "print(f'f1_socre = {f1_score(test_output, prediction_over_test)}')\n",
    "print(f'precision_score = {precision_score(test_output, prediction_over_test)}')\n",
    "print(f'recall_score = {recall_score(test_output, prediction_over_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9991397773954567"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(model_LR)\n",
    "cm.fit(train_input, train_output)\n",
    "cm.score(test_input, test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.04290866\n",
      "Iteration 2, loss = 0.00869957\n",
      "Iteration 3, loss = 0.00761672\n",
      "Iteration 4, loss = 0.00629233\n",
      "Iteration 5, loss = 0.00454601\n",
      "Iteration 6, loss = 0.00333434\n",
      "Iteration 7, loss = 0.00298248\n",
      "Iteration 8, loss = 0.00287963\n",
      "Iteration 9, loss = 0.00281005\n",
      "Iteration 10, loss = 0.00276709\n",
      "Iteration 11, loss = 0.00272076\n",
      "Iteration 12, loss = 0.00268055\n",
      "Iteration 13, loss = 0.00267327\n",
      "Iteration 14, loss = 0.00264586\n",
      "Iteration 15, loss = 0.00260722\n",
      "Iteration 16, loss = 0.00259401\n",
      "Iteration 17, loss = 0.00255488\n",
      "Iteration 18, loss = 0.00249681\n",
      "Iteration 19, loss = 0.00248218\n",
      "Iteration 20, loss = 0.00245007\n",
      "Iteration 21, loss = 0.00240087\n",
      "Iteration 22, loss = 0.00238236\n",
      "Iteration 23, loss = 0.00235334\n",
      "Iteration 24, loss = 0.00232586\n",
      "Iteration 25, loss = 0.00230891\n",
      "Iteration 26, loss = 0.00228931\n",
      "Iteration 27, loss = 0.00225645\n",
      "Iteration 28, loss = 0.00225374\n",
      "Iteration 29, loss = 0.00222429\n",
      "Iteration 30, loss = 0.00219482\n",
      "Iteration 31, loss = 0.00218518\n",
      "Iteration 32, loss = 0.00216875\n",
      "Iteration 33, loss = 0.00217382\n",
      "Iteration 34, loss = 0.00213069\n",
      "Iteration 35, loss = 0.00212833\n",
      "Iteration 36, loss = 0.00210576\n",
      "Iteration 37, loss = 0.00209050\n",
      "Iteration 38, loss = 0.00207265\n",
      "Iteration 39, loss = 0.00206885\n",
      "Iteration 40, loss = 0.00204430\n",
      "Iteration 41, loss = 0.00203342\n",
      "Iteration 42, loss = 0.00201977\n",
      "Iteration 43, loss = 0.00200101\n",
      "Iteration 44, loss = 0.00197957\n",
      "Iteration 45, loss = 0.00196143\n",
      "Iteration 46, loss = 0.00195038\n",
      "Iteration 47, loss = 0.00194510\n",
      "Iteration 48, loss = 0.00193675\n",
      "Iteration 49, loss = 0.00193083\n",
      "Iteration 50, loss = 0.00190086\n",
      "Iteration 51, loss = 0.00192024\n",
      "Iteration 52, loss = 0.00188763\n",
      "Iteration 53, loss = 0.00187810\n",
      "Iteration 54, loss = 0.00187338\n",
      "Iteration 55, loss = 0.00185111\n",
      "Iteration 56, loss = 0.00185932\n",
      "Iteration 57, loss = 0.00184063\n",
      "Iteration 58, loss = 0.00180611\n",
      "Iteration 59, loss = 0.00180797\n",
      "Iteration 60, loss = 0.00180147\n",
      "Iteration 61, loss = 0.00180698\n",
      "Iteration 62, loss = 0.00177407\n",
      "Iteration 63, loss = 0.00176730\n",
      "Iteration 64, loss = 0.00173568\n",
      "Iteration 65, loss = 0.00173787\n",
      "Iteration 66, loss = 0.00172253\n",
      "Iteration 67, loss = 0.00173814\n",
      "Iteration 68, loss = 0.00172041\n",
      "Iteration 69, loss = 0.00169194\n",
      "Iteration 70, loss = 0.00169424\n",
      "Iteration 71, loss = 0.00165957\n",
      "Iteration 72, loss = 0.00164056\n",
      "Iteration 73, loss = 0.00163592\n",
      "Iteration 74, loss = 0.00163654\n",
      "Iteration 75, loss = 0.00161601\n",
      "Iteration 76, loss = 0.00160069\n",
      "Iteration 77, loss = 0.00159325\n",
      "Iteration 78, loss = 0.00158587\n",
      "Iteration 79, loss = 0.00157311\n",
      "Iteration 80, loss = 0.00153727\n",
      "Iteration 81, loss = 0.00150499\n",
      "Iteration 82, loss = 0.00151020\n",
      "Iteration 83, loss = 0.00147541\n",
      "Iteration 84, loss = 0.00149410\n",
      "Iteration 85, loss = 0.00147787\n",
      "Iteration 86, loss = 0.00141168\n",
      "Iteration 87, loss = 0.00143004\n",
      "Iteration 88, loss = 0.00141624\n",
      "Iteration 89, loss = 0.00140392\n",
      "Iteration 90, loss = 0.00135650\n",
      "Iteration 91, loss = 0.00135717\n",
      "Iteration 92, loss = 0.00135873\n",
      "Iteration 93, loss = 0.00131519\n",
      "Iteration 94, loss = 0.00131805\n",
      "Iteration 95, loss = 0.00131212\n",
      "Iteration 96, loss = 0.00126275\n",
      "Iteration 97, loss = 0.00127751\n",
      "Iteration 98, loss = 0.00128001\n",
      "Iteration 99, loss = 0.00123902\n",
      "Iteration 100, loss = 0.00123515\n",
      "Iteration 101, loss = 0.00124687\n",
      "Iteration 102, loss = 0.00119946\n",
      "Iteration 103, loss = 0.00119812\n",
      "Iteration 104, loss = 0.00119246\n",
      "Iteration 105, loss = 0.00117406\n",
      "Iteration 106, loss = 0.00116512\n",
      "Iteration 107, loss = 0.00114498\n",
      "Iteration 108, loss = 0.00114348\n",
      "Iteration 109, loss = 0.00112594\n",
      "Iteration 110, loss = 0.00113800\n",
      "Iteration 111, loss = 0.00110916\n",
      "Iteration 112, loss = 0.00107883\n",
      "Iteration 113, loss = 0.00108958\n",
      "Iteration 114, loss = 0.00106781\n",
      "Iteration 115, loss = 0.00105179\n",
      "Iteration 116, loss = 0.00104932\n",
      "Iteration 117, loss = 0.00105422\n",
      "Iteration 118, loss = 0.00103590\n",
      "Iteration 119, loss = 0.00102247\n",
      "Iteration 120, loss = 0.00100789\n",
      "Iteration 121, loss = 0.00102416\n",
      "Iteration 122, loss = 0.00098713\n",
      "Iteration 123, loss = 0.00097592\n",
      "Iteration 124, loss = 0.00098797\n",
      "Iteration 125, loss = 0.00095129\n",
      "Iteration 126, loss = 0.00093349\n",
      "Iteration 127, loss = 0.00093622\n",
      "Iteration 128, loss = 0.00094063\n",
      "Iteration 129, loss = 0.00094126\n",
      "Iteration 130, loss = 0.00095116\n",
      "Iteration 131, loss = 0.00088341\n",
      "Iteration 132, loss = 0.00090974\n",
      "Iteration 133, loss = 0.00089334\n",
      "Iteration 134, loss = 0.00084291\n",
      "Iteration 135, loss = 0.00090688\n",
      "Iteration 136, loss = 0.00084224\n",
      "Iteration 137, loss = 0.00084760\n",
      "Iteration 138, loss = 0.00085219\n",
      "Iteration 139, loss = 0.00085322\n",
      "Iteration 140, loss = 0.00082335\n",
      "Iteration 141, loss = 0.00081770\n",
      "Iteration 142, loss = 0.00079902\n",
      "Iteration 143, loss = 0.00082233\n",
      "Iteration 144, loss = 0.00078573\n",
      "Iteration 145, loss = 0.00075635\n",
      "Iteration 146, loss = 0.00080430\n",
      "Iteration 147, loss = 0.00073109\n",
      "Iteration 148, loss = 0.00077300\n",
      "Iteration 149, loss = 0.00079263\n",
      "Iteration 150, loss = 0.00071664\n",
      "Iteration 151, loss = 0.00073643\n",
      "Iteration 152, loss = 0.00072267\n",
      "Iteration 153, loss = 0.00073729\n",
      "Iteration 154, loss = 0.00070970\n",
      "Iteration 155, loss = 0.00074433\n",
      "Iteration 156, loss = 0.00069104\n",
      "Iteration 157, loss = 0.00072999\n",
      "Iteration 158, loss = 0.00070079\n",
      "Iteration 159, loss = 0.00068358\n",
      "Iteration 160, loss = 0.00068473\n",
      "Iteration 161, loss = 0.00067222\n",
      "Iteration 162, loss = 0.00070262\n",
      "Iteration 163, loss = 0.00070891\n",
      "Iteration 164, loss = 0.00066027\n",
      "Iteration 165, loss = 0.00064939\n",
      "Iteration 166, loss = 0.00066275\n",
      "Iteration 167, loss = 0.00065916\n",
      "Iteration 168, loss = 0.00065918\n",
      "Iteration 169, loss = 0.00064969\n",
      "Iteration 170, loss = 0.00064492\n",
      "Iteration 171, loss = 0.00063369\n",
      "Iteration 172, loss = 0.00062512\n",
      "Iteration 173, loss = 0.00063762\n",
      "Iteration 174, loss = 0.00063941\n",
      "Iteration 175, loss = 0.00062340\n",
      "Iteration 176, loss = 0.00064094\n",
      "Iteration 177, loss = 0.00060957\n",
      "Iteration 178, loss = 0.00061000\n",
      "Iteration 179, loss = 0.00060216\n",
      "Iteration 180, loss = 0.00062168\n",
      "Iteration 181, loss = 0.00064220\n",
      "Iteration 182, loss = 0.00061826\n",
      "Iteration 183, loss = 0.00061122\n",
      "Iteration 184, loss = 0.00059306\n",
      "Iteration 185, loss = 0.00060449\n",
      "Iteration 186, loss = 0.00057175\n",
      "Iteration 187, loss = 0.00060414\n",
      "Iteration 188, loss = 0.00059480\n",
      "Iteration 189, loss = 0.00058776\n",
      "Iteration 190, loss = 0.00057511\n",
      "Iteration 191, loss = 0.00060337\n",
      "Iteration 192, loss = 0.00058925\n",
      "Iteration 193, loss = 0.00058860\n",
      "Iteration 194, loss = 0.00058398\n",
      "Iteration 195, loss = 0.00056890\n",
      "Iteration 196, loss = 0.00062116\n",
      "Iteration 197, loss = 0.00058399\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(25, 25), max_iter=1500,\n",
       "              tol=1e-05, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(25, 25), max_iter=1500,\n",
       "              tol=1e-05, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=(25, 25), max_iter=1500,\n",
       "              tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp = MLPClassifier(max_iter=1500, tol=0.00001, verbose=True, hidden_layer_sizes=(25,25), solver='adam', activation='logistic', random_state=1)\n",
    "model_mlp.fit(train_input, train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_over_test = model_mlp.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score = 0.9993504441557529\n",
      "f1_socre = 0.7730061349693252\n",
      "precision_score = 0.8289473684210527\n",
      "recall_score = 0.7241379310344828\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy_score = {accuracy_score(test_output, prediction_over_test)}')\n",
    "print(f'f1_socre = {f1_score(test_output, prediction_over_test)}')\n",
    "print(f'precision_score = {precision_score(test_output, prediction_over_test)}')\n",
    "print(f'recall_score = {recall_score(test_output, prediction_over_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993504441557529"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(model_mlp)\n",
    "cm.fit(train_input, train_output)\n",
    "cm.score(test_input, test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_mlp, open(model_path, 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_producer = pd.DataFrame(test_input, columns=input_columns)\n",
    "df_input_producer.to_csv(\"input_producer.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.318582</td>\n",
       "      <td>-0.460479</td>\n",
       "      <td>-0.107158</td>\n",
       "      <td>-0.162198</td>\n",
       "      <td>1.479558</td>\n",
       "      <td>-1.521378</td>\n",
       "      <td>0.251546</td>\n",
       "      <td>-0.451298</td>\n",
       "      <td>0.320564</td>\n",
       "      <td>0.063632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260489</td>\n",
       "      <td>-0.107992</td>\n",
       "      <td>0.058678</td>\n",
       "      <td>0.583635</td>\n",
       "      <td>0.041931</td>\n",
       "      <td>-4.277719</td>\n",
       "      <td>-0.422213</td>\n",
       "      <td>0.167403</td>\n",
       "      <td>0.740120</td>\n",
       "      <td>-0.341486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.422970</td>\n",
       "      <td>0.802057</td>\n",
       "      <td>0.881693</td>\n",
       "      <td>0.015947</td>\n",
       "      <td>-0.197957</td>\n",
       "      <td>-0.493717</td>\n",
       "      <td>0.275565</td>\n",
       "      <td>0.374776</td>\n",
       "      <td>-0.651301</td>\n",
       "      <td>-0.564972</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014363</td>\n",
       "      <td>-0.181804</td>\n",
       "      <td>-0.513412</td>\n",
       "      <td>0.140290</td>\n",
       "      <td>0.668144</td>\n",
       "      <td>-0.503967</td>\n",
       "      <td>0.162992</td>\n",
       "      <td>0.411195</td>\n",
       "      <td>0.205819</td>\n",
       "      <td>-0.340740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.167626</td>\n",
       "      <td>0.680815</td>\n",
       "      <td>0.641328</td>\n",
       "      <td>-0.093085</td>\n",
       "      <td>0.421605</td>\n",
       "      <td>-0.402303</td>\n",
       "      <td>0.593529</td>\n",
       "      <td>-0.056733</td>\n",
       "      <td>-0.430890</td>\n",
       "      <td>-0.497188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268708</td>\n",
       "      <td>-0.426112</td>\n",
       "      <td>-1.074502</td>\n",
       "      <td>-0.187609</td>\n",
       "      <td>-0.818748</td>\n",
       "      <td>-0.036235</td>\n",
       "      <td>0.248267</td>\n",
       "      <td>0.631343</td>\n",
       "      <td>0.315041</td>\n",
       "      <td>-0.345020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -0.318582 -0.460479 -0.107158 -0.162198  1.479558 -1.521378  0.251546   \n",
       "1 -0.422970  0.802057  0.881693  0.015947 -0.197957 -0.493717  0.275565   \n",
       "2 -0.167626  0.680815  0.641328 -0.093085  0.421605 -0.402303  0.593529   \n",
       "\n",
       "         V8        V9       V10  ...       V20       V21       V22       V23  \\\n",
       "0 -0.451298  0.320564  0.063632  ... -0.260489 -0.107992  0.058678  0.583635   \n",
       "1  0.374776 -0.651301 -0.564972  ... -0.014363 -0.181804 -0.513412  0.140290   \n",
       "2 -0.056733 -0.430890 -0.497188  ...  0.268708 -0.426112 -1.074502 -0.187609   \n",
       "\n",
       "        V24       V25       V26       V27       V28    Amount  \n",
       "0  0.041931 -4.277719 -0.422213  0.167403  0.740120 -0.341486  \n",
       "1  0.668144 -0.503967  0.162992  0.411195  0.205819 -0.340740  \n",
       "2 -0.818748 -0.036235  0.248267  0.631343  0.315041 -0.345020  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input_producer.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
